{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Q1. What is anomaly detection and what is its purpose?"
      ],
      "metadata": {
        "id": "0VYcPnzzWkFy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer:-\n",
        "\n",
        "Anomaly detection refers to the process of identifying data points or patterns that deviate significantly from the expected norm within a dataset. The main goal of this technique is to spot unusual occurrences that could indicate critical incidents, errors, or novel insights that warrant further investigation.\n",
        "\n",
        "Purpose of Anomaly Detection\n",
        "\n",
        "1.Detecting Unusual Events: The primary aim is to find rare or unexpected events that stand out from the regular data patterns. This is important in various fields, such as identifying fraud in financial transactions or spotting equipment failures in industrial settings.\n",
        "\n",
        "2.Enhancing Security: In the realm of cybersecurity, anomaly detection plays a vital role in identifying potential threats. By monitoring network traffic or user behavior, it can flag unusual activities that may suggest a security breach or attack.\n",
        "\n",
        "3.Quality Assurance: In manufacturing, this technique helps in maintaining product quality by identifying defects or irregularities in production processes. Early detection of these anomalies can prevent defective products from reaching consumers.\n",
        "\n",
        "4.System Monitoring: Anomaly detection is widely used in monitoring the performance of systems, such as servers or applications. It can help identify unusual patterns in metrics like CPU usage or memory consumption, which may indicate underlying issues.\n",
        "\n",
        "5.Fraud Prevention: In finance, it is crucial for detecting fraudulent activities. By analyzing spending patterns, anomaly detection can highlight transactions that deviate from a user’s typical behavior, prompting further investigation.\n",
        "\n",
        "6.Healthcare Insights: In healthcare, this approach can identify abnormal patient behaviors or medical conditions that require immediate attention, such as unusual vital signs or unexpected changes in patient data.\n",
        "\n",
        "7.Data Cleaning: Anomaly detection can also assist in refining datasets by identifying and removing outliers that could distort analysis or model training, leading to more reliable results.\n",
        "\n",
        "Conclusion\n",
        "\n",
        "In essence, anomaly detection is a valuable tool for identifying significant deviations in data. Its applications are diverse, spanning security, finance, manufacturing, and healthcare, making it essential for improving safety, quality, and operational efficiency. By recognizing anomalies, organizations can take timely actions to address potential issues and enhance overall performance."
      ],
      "metadata": {
        "id": "llHDpQDpWlG2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q2. What are the key challenges in anomaly detection?"
      ],
      "metadata": {
        "id": "w2pqybN2cPCS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer:-\n",
        "\n",
        "There are several key challenges in anomaly detection, including:\n",
        "\n",
        "1.Unlabeled data:\n",
        "\n",
        "In many cases, anomalies are rare and occur infrequently, making it difficult to have enough labeled data for model training. This makes it challenging to build accurate anomaly detection models.\n",
        "\n",
        "2.High dimensionality:\n",
        "\n",
        " Many datasets used in anomaly detection are high-dimensional, meaning they have a large number of features or variables. This can make it difficult to identify which features are most important and can lead to the \"curse of dimensionality\" problem.\n",
        "\n",
        "3.Data imbalance:\n",
        "\n",
        "Anomalies are often rare events, which can result in data imbalance, making it challenging to train models to detect them.\n",
        "\n",
        "4.Concept drift:\n",
        "\n",
        " The distribution of data can change over time, which can make previously trained models ineffective in detecting new anomalies. This is known as concept drift and requires frequent model retraining.\n",
        "\n",
        "5.Interpretability:\n",
        "\n",
        "Anomaly detection models can be complex and difficult to interpret, making it challenging to understand why a particular data point is flagged as an anomaly.\n",
        "\n",
        "6.False positives and false negatives:\n",
        "\n",
        " Anomaly detection models may generate false positives, flagging normal data as anomalies, or false negatives, missing actual anomalies, which can result in either costly false alarms or missed detections.\n",
        "\n",
        "Addressing these challenges requires advanced algorithms, feature engineering, appropriate data preprocessing, and a deep understanding of the domain and the data being analyzed."
      ],
      "metadata": {
        "id": "d1XOcK8EcUBH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q3. How does unsupervised anomaly detection differ from supervised anomaly detection?"
      ],
      "metadata": {
        "id": "WRun6Ie6ctp0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer:-\n",
        "\n",
        "Unsupervised anomaly detection and supervised anomaly detection are two different approaches to identifying unusual patterns in data, and they vary mainly in how they use labeled data and the techniques involved.\n",
        "\n",
        "1.Data Labeling\n",
        "\n",
        "Unsupervised Anomaly Detection:\n",
        "\n",
        "This method does not use labeled data. The algorithm analyzes the data to find patterns and identifies anomalies based on deviations from these patterns. It operates without prior knowledge of what constitutes an anomaly.\n",
        "\n",
        "Supervised Anomaly Detection:\n",
        "\n",
        "In this approach, the model is trained on a dataset that includes both normal and anomalous instances, meaning the data is labeled. The algorithm learns to distinguish between the two classes based on the features provided.\n",
        "\n",
        "2.Model Training\n",
        "\n",
        "Unsupervised Anomaly Detection:\n",
        "\n",
        "Techniques often involve clustering or statistical methods. The model identifies anomalies by looking for data points that do not fit well within the established patterns or clusters. Since there are no labels, the model relies on the data's inherent structure.\n",
        "\n",
        "Supervised Anomaly Detection:\n",
        "\n",
        "This method typically uses classification algorithms. The model learns from the labeled data to recognize the characteristics of normal and anomalous instances, allowing it to classify new data points accordingly.\n",
        "\n",
        "3.Performance Evaluation\n",
        "\n",
        "Unsupervised Anomaly Detection:\n",
        "\n",
        "Evaluating performance can be challenging because there are no labels to compare against. Metrics may include clustering quality or statistical measures, but these do not provide a clear picture of accuracy.\n",
        "\n",
        "Supervised Anomaly Detection:\n",
        "\n",
        "Performance can be assessed using standard metrics like accuracy, precision, recall, and F1 score, as there are known labels for both normal and anomalous instances. This makes it easier to evaluate how well the model is performing.\n",
        "\n",
        "4.Use Cases\n",
        "\n",
        "Unsupervised Anomaly Detection:\n",
        "\n",
        "This approach is useful in situations where labeled data is not available, such as detecting unusual patterns in network traffic, identifying fraud in transactions, or monitoring sensor data in various applications.\n",
        "\n",
        "Supervised Anomaly Detection:\n",
        "\n",
        "More appropriate when labeled data is accessible, such as in medical diagnostics or quality control processes, where anomalies can be clearly defined and labeled.\n",
        "\n",
        "Conclusion\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "In essence, the key difference between unsupervised and supervised anomaly detection lies in the use of labeled data. Unsupervised methods analyze data without labels to find anomalies, while supervised methods rely on labeled instances to train the model. Each approach has its own strengths and is suited to different scenarios based on the availability of data and the specific requirements of the task.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "39BQ1mCacuS4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q4. What are the main categories of anomaly detection algorithms?"
      ],
      "metadata": {
        "id": "4JsodzOceUzn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer:-\n",
        "\n",
        "The main categories of anomaly detection algorithms include:\n",
        "\n",
        "1.Statistical-based methods: These methods rely on statistical models to identify data points that deviate significantly from the expected behavior. Examples of statistical-based methods include Gaussian Mixture Models (GMM), Principal Component Analysis (PCA), and Time-series Analysis.\n",
        "\n",
        "2.Machine learning-based methods: These methods use machine learning algorithms to learn the patterns of normal behavior in the data and detect deviations from these patterns. Examples of machine learning-based methods include Support Vector Machines (SVM), Decision Trees, Random Forests, and Neural Networks.\n",
        "\n",
        "3.Clustering-based methods: These methods group similar data points together and identify data points that do not belong to any cluster as anomalies. Examples of clustering-based methods include K-means, DBSCAN, and Hierarchical Clustering.\n",
        "\n",
        "4.Density-based methods: These methods identify anomalies based on deviations from the density of the data. Examples of density-based methods include Local Outlier Factor (LOF), Isolation Forests, and Kernel Density Estimation.\n",
        "\n",
        "5.Rule-based methods: These methods define rules or thresholds that identify data points that fall outside of the expected range. Examples of rule-based methods include statistical process control and expert systems.\n",
        "\n",
        "Each category of anomaly detection methods has its strengths and weaknesses, and the appropriate method depends on the type of data, the context of the problem, and the specific requirements of the application."
      ],
      "metadata": {
        "id": "U9uaVkmNexOS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q5. What are the main assumptions made by distance-based anomaly detection methods?"
      ],
      "metadata": {
        "id": "kIJpx698gZcT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer:-\n",
        "\n",
        "Assumptions made while using Distance-based anomaly detection:\n",
        "\n",
        "1.Distance Metric: They rely on a distance metric (e.g., Euclidean distance) to measure similarity or dissimilarity between data points.\n",
        "\n",
        "2.Spherical Clusters: They assume that clusters are roughly spherical or have similar densities in all directions.\n",
        "\n",
        "3.Constant Density: Some assume constant density within clusters, which may not hold for varying-density clusters.\n",
        "\n",
        "4.Symmetry: They assume symmetric distances, which means the distance from A to B is the same as from B to A.\n",
        "\n",
        "5.Independence: They assume independence of attributes, which may not hold for correlated features.\n",
        "\n",
        "6.Homogeneous Data: They assume all normal data points belong to the same distribution.\n",
        "\n",
        "7.Single Scale: They treat all attributes equally, which can be problematic with varying scales.\n",
        "\n",
        "8.Noisy Data: They may struggle to distinguish anomalies from noisy data.\n",
        "\n",
        "9.Known Clusters: Some require specifying the number of clusters in advance."
      ],
      "metadata": {
        "id": "kZn_O8u0gZ6V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q6. How does the LOF algorithm compute anomaly scores?"
      ],
      "metadata": {
        "id": "VXqTQLH1hRBN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer:-\n",
        "\n",
        "The Local Outlier Factor (LOF) algorithm computes anomaly scores by measuring the local density of a data point with respect to its neighbors. The LOF algorithm identifies data points that have a significantly lower density than their neighbors as anomalies.\n",
        "\n",
        "The algorithm works as follows:\n",
        "\n",
        "1.For each data point, identify its k nearest neighbors based on a distance metric (e.g., Euclidean distance). The value of k is a user-defined parameter.\n",
        "\n",
        "2.Compute the local reachability density (LRD) of the data point as the inverse of the average distance between the data point and its k nearest neighbors.\n",
        "\n",
        "3.Compute the local outlier factor (LOF) of the data point as the average LRD of its k nearest neighbors divided by its own LRD.\n",
        "\n",
        "4.Anomalies are identified as data points with an LOF score that is significantly higher than the LOF scores of their neighbors. The threshold for identifying anomalies is also a user-defined parameter.\n",
        "\n",
        "Intuitively, the LOF algorithm computes the density of a data point in relation to its neighbors and identifies data points that are significantly less dense than their neighbors as anomalies. This approach can detect anomalies that are not isolated, but rather exist in low-density regions of the data.\n",
        "\n",
        "The LOF algorithm is widely used in anomaly detection applications and is especially useful for datasets with non-uniform density distributions."
      ],
      "metadata": {
        "id": "2fyJZarUhRkc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q7. What are the key parameters of the Isolation Forest algorithm?"
      ],
      "metadata": {
        "id": "EoP0Rz1bhsmb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer:-\n",
        "\n",
        "The Isolation Forest algorithm has several key parameters that play a crucial role in its performance. Here’s a breakdown of these parameters:\n",
        "\n",
        "1.n_estimators\n",
        "\n",
        "This parameter defines the number of isolation trees to be created in the forest. More trees can lead to better performance and more reliable results, but they also increase the computational cost. A common choice is between 100 and 200 trees.\n",
        "\n",
        "2.max_samples\n",
        "\n",
        "This specifies the number of samples to draw from the dataset to build each isolation tree. It can be set as an integer (specific number of samples) or a float (percentage of the total dataset). A smaller sample size can speed up the process but may affect the model's accuracy.\n",
        "\n",
        "3.contamination\n",
        "\n",
        "This parameter indicates the expected proportion of anomalies in the dataset. It helps the algorithm determine the threshold for classifying points as anomalies. Setting this correctly is important for effective anomaly detection.\n",
        "\n",
        "4.max_features\n",
        "\n",
        "This parameter controls the number of features to consider when looking for the best split at each node of the trees. It can be set as an integer or a float. Limiting the number of features can help prevent overfitting, especially in datasets with many dimensions.\n",
        "\n",
        "5.bootstrap\n",
        "\n",
        "This is a boolean parameter that indicates whether to use bootstrap sampling when building the trees. If set to True, samples are drawn with replacement; if False, they are drawn without replacement. This can affect the variability and robustness of the model.\n",
        "\n",
        "6.random_state\n",
        "\n",
        "This parameter sets the seed for the random number generator, which ensures that the results are reproducible. It’s useful for consistency when running the algorithm multiple times.\n",
        "\n",
        "Conclusion\n",
        "\n",
        "These parameters—n_estimators, max_samples, contamination, max_features, bootstrap, and random_state—are essential for tuning the Isolation Forest algorithm. Adjusting them appropriately can enhance the model's ability to detect anomalies effectively."
      ],
      "metadata": {
        "id": "JULNwZLshtXG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q8. If a data point has only 2 neighbours of the same class within a radius of 0.5, what is its anomaly score\n",
        "using KNN with K=10?"
      ],
      "metadata": {
        "id": "gqiEAmo-iiOs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer:-\n",
        "\n",
        "In K-Nearest Neighbors (KNN) for anomaly detection, the anomaly score of a data point is typically determined by measuring the distance between that data point and its k-nearest neighbors. Anomalies are often identified as data points with neighbors that are significantly farther away from them in comparison to the majority of data points.\n",
        "\n",
        "In this case, the data point has only 2 neighbors of the same class within a radius of 0.5. Since K=10, we need to find the distance between the data point and its 10th nearest neighbor. If the data point has only 2 neighbors within a radius of 0.5, it is unlikely that it will have 10 neighbors within the same radius. Therefore, we cannot compute the anomaly score of the data point using KNN with K=10.\n",
        "\n",
        "However, if we still want to compute the anomaly score using KNN with K=10, we can extend the distance radius until we find 10 neighbors.\n",
        "\n",
        "For example, if we extend the radius to 1, we may find 10 neighbors. We can then compute the distance between the data point and its 10th nearest neighbor and use it to compute the anomaly score. The larger the distance, the higher the anomaly score.\n",
        "\n",
        "Anomaly Score = 1 / (average distance to k nearest neighbors)"
      ],
      "metadata": {
        "id": "chf-eButiiuW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q9. Using the Isolation Forest algorithm with 100 trees and a dataset of 3000 data points, what is the\n",
        "anomaly score for a data point that has an average path length of 5.0 compared to the average path\n",
        "length of the trees?"
      ],
      "metadata": {
        "id": "BZkRwC18ixN1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer:-\n",
        "\n",
        "The Isolation Forest algorithm generates a forest of decision trees, where each data point is isolated in a different partition of the feature space. The anomaly score of a data point is computed based on the average path length of the data point in the trees of the forest.\n",
        "\n",
        "If a data point has an average path length of 5.0 compared to the average path length of the trees, we can compute its anomaly score using the following\n",
        "formula:\n",
        "\n",
        "Anomaly Score = 2^(-average path length / c(n))\n",
        "\n",
        "where c(n) is a constant that depends on the number of data points n in the dataset. The value of c(n) can be computed as:\n",
        "\n",
        "c(n) = 2 * H(n-1) - (2 * (n-1) / n)\n",
        "\n",
        "where H(n-1) is the harmonic number of n-1.\n",
        "\n",
        "For a dataset of 3000 data points, c(n) can be computed as:\n",
        "\n",
        "c(3000) = 2 * H(2999) - (2 * 2999 / 3000) = 11.8979\n",
        "\n",
        "Using this value of c(n), we can compute the anomaly score of the data point with an average path length of 5.0 as:\n",
        "\n",
        "Anomaly Score = 2^(-5.0 / 11.8979) = 0.5017\n",
        "\n",
        "This indicates that the data point is less anomalous than a data point with an average path length that is farther from the average path length of the trees.\n",
        "\n",
        "Below is manual method to calulate anomaly score in python :"
      ],
      "metadata": {
        "id": "s9WbBoBMixqF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import IsolationForest\n",
        "import numpy as np\n",
        "\n",
        "# Generate a dataset of 3000 data points with 10 features\n",
        "X = np.random.randn(3000, 10)\n",
        "\n",
        "# Fit an Isolation Forest model with 100 trees\n",
        "clf = IsolationForest(n_estimators=100, contamination='auto', random_state=42)\n",
        "clf.fit(X)\n",
        "\n",
        "# Compute the average path length of a data point with 5.0 compared to the average path length of the trees\n",
        "avg_path_length = 5.0\n",
        "tree_path_lengths = np.zeros(clf.n_estimators)\n",
        "for i, tree in enumerate(clf.estimators_):\n",
        "    path = tree.decision_path(X)\n",
        "    tree_path_lengths[i] = path.indices.size - 1\n",
        "avg_tree_path_length = np.mean(tree_path_lengths)\n",
        "\n",
        "# Compute the anomaly score using the formula for Isolation Forest\n",
        "c = 2 * np.log(X.shape[0] - 1) - (2 * (X.shape[0] - 1) / X.shape[0])\n",
        "anomaly_score = 2 ** (-avg_path_length / c)\n",
        "\n",
        "print(f\"The anomaly score of the data point is {anomaly_score:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oG1IgEigj7gN",
        "outputId": "2eb2aea0-fd54-4029-b9da-db5b0d469482"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The anomaly score of the data point is 0.7809\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Below is sklearn score_samples method to find anomaly_scores\n",
        "\n",
        "from sklearn.ensemble import IsolationForest\n",
        "import numpy as np\n",
        "\n",
        "# Generate a dataset of 3000 data points with 10 features\n",
        "X = np.random.randn(3000, 10)\n",
        "\n",
        "# Fit an Isolation Forest model with 100 trees\n",
        "clf = IsolationForest(n_estimators=100, contamination='auto', random_state=42)\n",
        "clf.fit(X)\n",
        "\n",
        "# Compute the anomaly scores for the data points\n",
        "anomaly_scores = clf.score_samples(X)\n",
        "\n",
        "# Print the anomaly scores\n",
        "print(anomaly_scores)\n",
        "\n",
        "\n",
        "# Compute the mean of the anomaly scores\n",
        "mean_anomaly_score = np.mean(anomaly_scores)\n",
        "\n",
        "# Print the mean anomaly score\n",
        "print(f\"\\nThe mean anomaly score is {mean_anomaly_score:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vV8_ywrrj_x4",
        "outputId": "cdb4702d-eeac-47a8-97e2-baafe474587e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[-0.41463693 -0.43542591 -0.46502742 ... -0.4436538  -0.43042695\n",
            " -0.46900303]\n",
            "\n",
            "The mean anomaly score is -0.4404\n"
          ]
        }
      ]
    }
  ]
}